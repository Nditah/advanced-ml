{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Advanced Robotics, 2024-2025\n# Dr Paul Baxter\n# Workshop Week 2\n\nCode based on example: https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/GridWorld/gridWorld.py\nwith description: https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff\n\nRun with: `python3 lecture2-simulation.py`\n\nMay need following dependencies:\n  `python -mpip install numpy matplotlib`\n\n\nThree classes in this notebook:\n 1. `State`: the board/maze\n 2. `Agent`: a basic learning agent (state value iteration)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# global variables\nLOGGING = False         #set full logging to terminal or not...\nL_ITERATIONS = 50       #number of learning iterations for Agent\nEXPLORE = 0.3           #the explore proportion: (1-EXPLORE) for exloit\n\n# maze states - leave alone for now...\nBOARD_ROWS = 3\nBOARD_COLS = 4\nWIN_STATE = (0, 3)\nLOSE_STATE = (1, 3)\nSTART = (2, 0)          #third row, first column"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## The maze environment"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": "class State:\n    def __init__(self, state=START):\n        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n        self.board[1, 1] = -1\n        self.state = state\n        self.isEnd = False\n\n    def giveReward(self):\n        if self.state == WIN_STATE:\n            return 1\n        elif self.state == LOSE_STATE:\n            return -1\n        else:\n            return 0\n\n    def isEndFunc(self):\n        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n            self.isEnd = True\n\n    def nxtPosition(self, action):\n        \"\"\"\n        action: up, down, left, right\n        -------------\n        0 | 1 | 2| 3|\n        1 |\n        2 |\n        return next position\n        \"\"\"\n        if action == \"up\":\n            nxtState = (self.state[0] - 1, self.state[1])\n        elif action == \"down\":\n            nxtState = (self.state[0] + 1, self.state[1])\n        elif action == \"left\":\n            nxtState = (self.state[0], self.state[1] - 1)\n        else:\n            nxtState = (self.state[0], self.state[1] + 1)\n        # if next state legal\n        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n                if nxtState != (1, 1):\n                    return nxtState\n        return self.state\n\n    def showBoard(self):\n        self.board[self.state] = 1\n        for i in range(0, BOARD_ROWS):\n            print('-----------------')\n            out = '| '\n            for j in range(0, BOARD_COLS):\n                if self.board[i, j] == 1:\n                    token = '*'\n                if self.board[i, j] == -1:\n                    token = 'X'\n                if self.board[i, j] == 0:\n                    token = '0'\n                out += token + ' | '\n            print(out)\n        print('-----------------')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Agent using basic value iteration"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": "class Agent:\n\n    def __init__(self):\n        self.states = []\n        self.numStates = []\n        self.rewards = []\n        self.cumulativeReward = []\n        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n        self.State = State()\n        self.lr = 0.2\n        self.exp_rate = EXPLORE\n        self.mean_moves = 0.0\n\n        # initial state reward\n        self.state_values = {}\n        for i in range(BOARD_ROWS):\n            for j in range(BOARD_COLS):\n                self.state_values[(i, j)] = 0  # set initial value to 0\n\n    def chooseAction(self):\n        # choose action with most expected value\n        mx_nxt_reward = 0\n        action = \"\"\n\n        if np.random.uniform(0, 1) <= self.exp_rate:\n            action = np.random.choice(self.actions)\n        else:\n            # greedy action\n            for a in self.actions:\n                nxt_reward = self.state_values[self.State.nxtPosition(a)]\n                if nxt_reward >= mx_nxt_reward:\n                    action = a\n                    mx_nxt_reward = nxt_reward\n        return action\n\n    def takeAction(self, action):\n        position = self.State.nxtPosition(action)\n        return State(state=position)\n\n    def reset(self):\n        self.states = []\n        self.State = State()\n\n    def play(self, rounds=10):\n        i = 0\n        print (\"\")\n        print (\"LEARNING START\")\n        stepCounter = 0\n        while i < rounds:\n            # to the end of game back propagate reward\n            if self.State.isEnd:\n                # back propagate\n                reward = self.State.giveReward()\n                self.rewards.append(reward)\n                # explicitly assign end state to reward values\n                self.state_values[self.State.state] = reward  # this is optional\n                print (\"Episode \", i, \": \")\n                print (\"--------------------- End Reward\", reward)\n                print (\"--------------------- Num Steps Used: \", stepCounter)\n                for s in reversed(self.states):\n                    reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n                    self.state_values[s] = round(reward, 3)\n                self.reset()\n                self.numStates.append(stepCounter)\n                stepCounter = 0\n                i += 1\n            else:\n                stepCounter += 1\n                action = self.chooseAction()\n                # append trace\n                self.states.append(self.State.nxtPosition(action))\n                if (LOGGING):\n                    print(\"  current position {} action {}\".format(self.State.state, action))\n                # by taking the action, it reaches the next state\n                self.State = self.takeAction(action)\n                # mark is end\n                self.State.isEndFunc()\n                if (LOGGING):\n                    print (\"    |--> next state\", self.State.state)\n\n    def showValues(self):\n        print (\"\")\n        for i in range(0, BOARD_ROWS):\n            print (\"-------------------------------------\")\n            out = '| '\n            for j in range(0, BOARD_COLS):\n                out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n            print(out)\n        print (\"-------------------------------------\")\n        "
    }
  ]
}

